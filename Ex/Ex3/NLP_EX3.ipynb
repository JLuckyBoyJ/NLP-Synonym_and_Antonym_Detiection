{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_EX3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy"
      ],
      "metadata": {
        "id": "jhcfiLuTofDz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import keras.backend as K\n"
      ],
      "metadata": {
        "id": "sKET4pW613BA"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CY5e2pqP1Wq_",
        "outputId": "36db0cfe-8bf2-40c6-decf-e1762f3def30"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Input\n",
        "\n"
      ],
      "metadata": {
        "id": "I1_QymOg1nL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read embedding\n"
      ],
      "metadata": {
        "id": "9bK-de3J96eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = pd.read_csv('/content/drive/MyDrive/Colab/NLP/Word-Similarity/word2vec/e300.csv')\n",
        "embedding = w2v.iloc[:, : 300].to_numpy()\n",
        "embedding = dict(zip(w2v['word'],embedding))"
      ],
      "metadata": {
        "id": "IKkrTryvpX5n"
      },
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read test"
      ],
      "metadata": {
        "id": "7AIvU33x9-XE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "make_unique = {}\n",
        "def readTest():\n",
        "  vicon400X_noun = []\n",
        "  vicon400Y_noun = []\n",
        "  word_pair = []\n",
        "  word_pair_noun = []\n",
        "  word_pair_verb = []\n",
        "  word_pair_adj = []\n",
        "  with open('/content/drive/MyDrive/Colab/NLP/Word-Similarity/datasets/ViCon-400/400_noun_pairs.txt') as f:\n",
        "      line = f.readline()\n",
        "      for i in range(400):\n",
        "          line = f.readline()\n",
        "          es = line.split()\n",
        "          if len(es) > 2:\n",
        "              vicon400X_noun.append(np.concatenate([embedding[es[0]],embedding[es[1]]]))\n",
        "              make_unique[es[0]+es[1]] = 1\n",
        "              word_pair_noun.append((es[0],es[1]))\n",
        "              if es[2][0]=='A':\n",
        "                  vicon400Y_noun.append(0)\n",
        "              else :\n",
        "                  vicon400Y_noun.append(1)\n",
        "        \n",
        "  vicon400X_verb = []\n",
        "  vicon400Y_verb = []\n",
        "  with open('/content/drive/MyDrive/Colab/NLP/Word-Similarity/datasets/ViCon-400/400_verb_pairs.txt') as f:\n",
        "      line = f.readline()\n",
        "      for i in range(400):\n",
        "          line = f.readline()\n",
        "          es = line.split()\n",
        "          if len(es) > 2:\n",
        "              make_unique[es[0]+es[1]] = 1\n",
        "              vicon400X_verb.append(np.concatenate([embedding[es[0]],embedding[es[1]]]))\n",
        "              word_pair_verb.append((es[0],es[1]))\n",
        "              if es[2][0]=='A':\n",
        "                  vicon400Y_verb.append(0)\n",
        "              else :\n",
        "                  vicon400Y_verb.append(1)\n",
        "\n",
        "  vicon400X_adj = []\n",
        "  vicon400Y_adj = []\n",
        "  with open('/content/drive/MyDrive/Colab/NLP/Word-Similarity/datasets/ViCon-400/600_adj_pairs.txt') as f:\n",
        "      line = f.readline()\n",
        "      for i in range(600):\n",
        "          line = f.readline()\n",
        "          es = line.split()\n",
        "          if len(es) > 2:\n",
        "              make_unique[es[0]+es[1]] = 1\n",
        "              vicon400X_adj.append(np.concatenate([embedding[es[0]],embedding[es[1]]]))\n",
        "              word_pair_adj.append((es[0],es[1]))\n",
        "              if es[2][0]=='A':\n",
        "                  vicon400Y_adj.append(0)\n",
        "              else :\n",
        "                  vicon400Y_adj.append(1)\n",
        "\n",
        "  vicon400X = vicon400X_noun + vicon400X_verb + vicon400X_adj\n",
        "  vicon400Y = vicon400Y_noun + vicon400Y_verb + vicon400Y_adj\n",
        "\n",
        "  vicon400X_noun = np.array(vicon400X_noun)\n",
        "  vicon400Y_noun = np.array(vicon400Y_noun)\n",
        "  \n",
        "  vicon400X_verb = np.array(vicon400X_verb)\n",
        "  vicon400Y_verb = np.array(vicon400Y_verb)\n",
        "  \n",
        "  vicon400X_adj = np.array(vicon400X_adj)\n",
        "  vicon400Y_adj = np.array(vicon400Y_adj)\n",
        "\n",
        "  vicon400X = np.array(vicon400X)\n",
        "  vicon400Y = np.array(vicon400Y)\n",
        "\n",
        "  return vicon400X_noun,vicon400Y_noun,vicon400X_verb,vicon400Y_verb,vicon400X_adj,vicon400Y_adj , vicon400X,vicon400Y , word_pair_noun,word_pair_verb,word_pair_adj\n",
        "\n",
        "vicon400X_noun,vicon400Y_noun,vicon400X_verb,vicon400Y_verb,vicon400X_adj,vicon400Y_adj , vicon400X,vicon400Y ,  word_pair_noun,word_pair_verb,word_pair_adj = readTest()"
      ],
      "metadata": {
        "id": "xKSIXcfuf_bY"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Train\n",
        "Có  2 tùy chọn:\n",
        "> *isFull* để đọc dữ liệu sau khi đã dùng phương pháp Oversampling  \n",
        "> *isContaintTest* để loại bỏ những dữ liệu trong tập *train* đã có trong tập *test*"
      ],
      "metadata": {
        "id": "YZ5hokI7-BcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readInput(isFull=False, isContaintTest=True):\n",
        "  path_a = \"/content/drive/MyDrive/Colab/NLP/Word-Similarity/antonym-synonym set/Antonym_vietnamese.txt\"\n",
        "  path_s = \"/content/drive/MyDrive/Colab/NLP/Word-Similarity/antonym-synonym set/Synonym_vietnamese.txt\"\n",
        "  if isFull:\n",
        "      path_a = \"/content/drive/MyDrive/Colab/NLP/Word-Similarity/antonym-synonym set/Antonym_vietnamese_full.txt\"\n",
        "      path_s = \"/content/drive/MyDrive/Colab/NLP/Word-Similarity/antonym-synonym set/Synonym_vietnamese_full.txt\"\n",
        "  X = []\n",
        "  Y = []\n",
        "  with open(path_a) as f:\n",
        "      N = 2000\n",
        "      if isFull:\n",
        "        N = 5921\n",
        "      for i in range(N):\n",
        "          line = f.readline()\n",
        "          elements = line.split()\n",
        "          if(elements[0]==elements[1]):\n",
        "            continue\n",
        "          if elements[0] in embedding and elements[1] in embedding and elements[0]+elements[1] not in make_unique:\n",
        "              X.append(np.concatenate([embedding[elements[0]],embedding[elements[1]]]))\n",
        "              Y.append(0)\n",
        "\n",
        "  with open(path_s) as f:\n",
        "      N = 11526\n",
        "      if isFull:\n",
        "          N = 51543\n",
        "      for i in range(N):\n",
        "          line = f.readline()\n",
        "          elements = line.split()\n",
        "          if len(elements) < 2 or elements[0]==elements[1]:\n",
        "            continue\n",
        "          if elements[0] in embedding and elements[1] in embedding and elements[0]+elements[1] not in make_unique:\n",
        "              X.append(np.concatenate([embedding[elements[0]],embedding[elements[1]]]))\n",
        "              Y.append(1)\n",
        "  X = np.array(X)\n",
        "  Y = np.array(Y)\n",
        "  return X,Y\n",
        "\n",
        "X,Y = readInput(isFull=True,isContaintTest = False)"
      ],
      "metadata": {
        "id": "BIzuUbQ1-lAL"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split train\n"
      ],
      "metadata": {
        "id": "Ql1AetbC-GJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "       X, Y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Uio8NyLBCjkn"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n"
      ],
      "metadata": {
        "id": "Y6GwKl2w-J09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f1 score\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "metadata": {
        "id": "PJR6Jp6Qm_90"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Dense(100, activation = 'relu'),\n",
        "                               tf.keras.layers.Dense(10, activation = 'relu'),\n",
        "                               tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "                              ])\n",
        "model.compile( loss= tf.keras.losses.binary_crossentropy,\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01),\n",
        "                metrics = [get_f1])"
      ],
      "metadata": {
        "id": "RAI7-NLXCqZb"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(X_train,y_train)\n",
        "# loss, accuracy = model.evaluate(X_test, y_test)\n",
        "# print(f' Model loss on the test set: {loss}')\n",
        "# print(f' Model accuracy on the test set: {100*accuracy}')"
      ],
      "metadata": {
        "id": "A8AI3FSvMMkT"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.summary()"
      ],
      "metadata": {
        "id": "ySfAbOBo2XFi"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and evaluate\n",
        " \n"
      ],
      "metadata": {
        "id": "hwc5PUMC9YAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Dense(100, activation = 'relu'),\n",
        "                               tf.keras.layers.Dense(10, activation = 'relu'),\n",
        "                               tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "                              ])\n",
        "model.compile( loss= tf.keras.losses.binary_crossentropy,\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01),\n",
        "                metrics = [get_f1])\n",
        "model.fit(X, Y, epochs = 35, verbose = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tpmHrVrFy3G",
        "outputId": "f55b5b01-60a6-4e9b-d01d-828f16b020a9"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "1327/1327 [==============================] - 5s 3ms/step - loss: 0.2593 - get_f1: 0.9413\n",
            "Epoch 2/35\n",
            "1327/1327 [==============================] - 5s 3ms/step - loss: 0.1591 - get_f1: 0.9632\n",
            "Epoch 3/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.1142 - get_f1: 0.9740\n",
            "Epoch 4/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0913 - get_f1: 0.9790\n",
            "Epoch 5/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0755 - get_f1: 0.9825\n",
            "Epoch 6/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0629 - get_f1: 0.9848\n",
            "Epoch 7/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0580 - get_f1: 0.9858\n",
            "Epoch 8/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0482 - get_f1: 0.9883\n",
            "Epoch 9/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0456 - get_f1: 0.9889\n",
            "Epoch 10/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0454 - get_f1: 0.9894\n",
            "Epoch 11/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0389 - get_f1: 0.9910\n",
            "Epoch 12/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0361 - get_f1: 0.9916\n",
            "Epoch 13/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0338 - get_f1: 0.9924\n",
            "Epoch 14/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0325 - get_f1: 0.9928\n",
            "Epoch 15/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0284 - get_f1: 0.9936\n",
            "Epoch 16/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0279 - get_f1: 0.9938\n",
            "Epoch 17/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0264 - get_f1: 0.9939\n",
            "Epoch 18/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0275 - get_f1: 0.9936\n",
            "Epoch 19/35\n",
            "1327/1327 [==============================] - 5s 3ms/step - loss: 0.0219 - get_f1: 0.9949\n",
            "Epoch 20/35\n",
            "1327/1327 [==============================] - 5s 4ms/step - loss: 0.0253 - get_f1: 0.9942\n",
            "Epoch 21/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0229 - get_f1: 0.9948\n",
            "Epoch 22/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0239 - get_f1: 0.9942\n",
            "Epoch 23/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0213 - get_f1: 0.9952\n",
            "Epoch 24/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0239 - get_f1: 0.9950\n",
            "Epoch 25/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0231 - get_f1: 0.9951\n",
            "Epoch 26/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0181 - get_f1: 0.9955\n",
            "Epoch 27/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0201 - get_f1: 0.9952\n",
            "Epoch 28/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0218 - get_f1: 0.9952\n",
            "Epoch 29/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0181 - get_f1: 0.9956\n",
            "Epoch 30/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0178 - get_f1: 0.9961\n",
            "Epoch 31/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0153 - get_f1: 0.9968\n",
            "Epoch 32/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0154 - get_f1: 0.9964\n",
            "Epoch 33/35\n",
            "1327/1327 [==============================] - 5s 3ms/step - loss: 0.0158 - get_f1: 0.9958\n",
            "Epoch 34/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0175 - get_f1: 0.9959\n",
            "Epoch 35/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0140 - get_f1: 0.9964\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f61ef212e10>"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzyYwkhyVrGO",
        "outputId": "22cb48ff-a068-4a31-bab9-c22b0eaaf834"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_36\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_112 (Dense)           (None, 100)               60100     \n",
            "                                                                 \n",
            " dense_113 (Dense)           (None, 10)                1010      \n",
            "                                                                 \n",
            " dense_114 (Dense)           (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61,121\n",
            "Trainable params: 61,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "#model.save(\"/content/drive/MyDrive/Colab/NLP/Word-Similarity/model_oversampling_data_300_dim\")"
      ],
      "metadata": {
        "id": "XVsbx4IOHT0t"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "#model = keras.models.load_model(\"/content/drive/MyDrive/Colab/NLP/Word-Similarity/model_oversampling_data_300_dim\",custom_objects={'get_f1':get_f1})"
      ],
      "metadata": {
        "id": "wdFfk5FtHoS0"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(vicon400X, vicon400Y,model):\n",
        "  loss, accuracy = model.evaluate(vicon400X, vicon400Y)\n",
        "  # print(f' Model loss on the test set: {loss}')\n",
        "  print(f'Model accuracy on the test set: {100*accuracy}')\n",
        "  print('-'* 100)\n",
        "  y_pred = model.predict(vicon400X)\n",
        "  y_pred_pro = copy.deepcopy(y_pred)\n",
        "  y_pred[y_pred>0.5] = 1\n",
        "  y_pred[y_pred<=0.5] = 0\n",
        "  print('Confusion matix :\\n')\n",
        "  print(confusion_matrix(vicon400Y, y_pred))\n",
        "\n",
        "  print('-' * 100) \n",
        "  precision, recall, fscore, support = score(vicon400Y, y_pred,average='macro')\n",
        "  print('precision: {}'.format(precision))\n",
        "  print('recall: {}'.format(recall))\n",
        "  print('f1score: {}'.format(fscore))\n",
        "  # print('support: {}'.format(support))\n",
        "evaluate(vicon400X, vicon400Y,model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn1fsKfrsY0k",
        "outputId": "b9087e0c-4235-42ed-b04b-3dde1c9f6241"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44/44 [==============================] - 0s 3ms/step - loss: 1.2357 - get_f1: 0.8768\n",
            "Model accuracy on the test set: 87.68333792686462\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Confusion matix :\n",
            "\n",
            "[[535 165]\n",
            " [ 26 674]]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "precision: 0.8784957476326754\n",
            "recall: 0.8635714285714285\n",
            "f1score: 0.8622131738427632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0K9Y4Ve8gnWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# đánh giá kết quả trên các loại từ \n",
        "def evaluate_each_type(vicon400X, vicon400Y , model , type):     \n",
        "  loss, accuracy = model.evaluate(vicon400X, vicon400Y)\n",
        "  # print(f' Model loss on the test set: {loss}')\n",
        "  print(f'Type: {type}')  \n",
        "  print('-'* 100)\n",
        "  y_pred = model.predict(vicon400X)\n",
        "  y_pred_pro = copy.deepcopy(y_pred)\n",
        "  y_pred[y_pred>0.5] = 1\n",
        "  y_pred[y_pred<=0.5] = 0\n",
        "  word_pair = []\n",
        "  if type =='noun' :\n",
        "      word_pair = word_pair_noun\n",
        "  elif type == 'verb':\n",
        "      word_pair = word_pair_verb\n",
        "  else :\n",
        "      word_pair = word_pair_adj\n",
        "\n",
        "  precision, recall, fscore, support = precision_recall_fscore_support(vicon400Y, y_pred,average='macro')\n",
        "  print('precision: {}'.format(precision))\n",
        "  print('recall: {}'.format(recall))\n",
        "  print('f1score: {}'.format(fscore))\n",
        "\n",
        "  # debug step\n",
        "  print('-'*100)\n",
        "  print('wrong prediction:')\n",
        "  diff = vicon400Y - y_pred.reshape(-1)\n",
        "  tword_pair = np.array(word_pair)\n",
        "  vicon400Y=vicon400Y.reshape(len(vicon400Y),1)\n",
        "  debug_arr = np.concatenate(  (tword_pair[np.where(diff!=0)] , vicon400Y[np.where(diff!=0)]),axis=1 )\n",
        "  print(debug_arr)"
      ],
      "metadata": {
        "id": "Kum_3PZrvbgv"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate with noun"
      ],
      "metadata": {
        "id": "YOFd9agSp-P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate with noun\n",
        "evaluate_each_type(vicon400X_noun, vicon400Y_noun,model,'noun')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4nlsM1BXG7h",
        "outputId": "bf8e18c4-285f-4bca-9379-5d27b44cd7b9"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 3ms/step - loss: 2.5136 - get_f1: 0.8004\n",
            "Type: noun\n",
            "----------------------------------------------------------------------------------------------------\n",
            "precision: 0.8140046054008792\n",
            "recall: 0.7625\n",
            "f1score: 0.7523445799307868\n",
            "----------------------------------------------------------------------------------------------------\n",
            "wrong prediction:\n",
            "[['khoái_lạc' 'nỗi_đau' '0']\n",
            " ['thanh_danh' 'ô_nhục' '0']\n",
            " ['ba' 'me' '0']\n",
            " ['ngày' 'ban_đêm' '0']\n",
            " ['khuyết_thiếu' 'ưu_điểm' '0']\n",
            " ['bệ' 'đầu' '0']\n",
            " ['thầy' 'u' '0']\n",
            " ['dĩ_vãng' 'tương_lai' '0']\n",
            " ['đàn_bà' 'nam_giới' '0']\n",
            " ['xuân_phân' 'thu_phân' '0']\n",
            " ['nữ_hoàng' 'vua' '0']\n",
            " ['nam' 'trai' '1']\n",
            " ['phụ_thân' 'má' '0']\n",
            " ['thiếu_nữ' 'chàng_trai' '0']\n",
            " ['sở_trường' 'nhược_điểm' '0']\n",
            " ['đàn_bà' 'nam' '0']\n",
            " ['tinh_mơ' 'hoàng_hôn' '0']\n",
            " ['tiểu_phẫu' 'đại_phẫu' '0']\n",
            " ['quá_khứ' 'vị_lai' '0']\n",
            " ['thầy' 'me' '0']\n",
            " ['dĩ_vãng' 'sau_này' '0']\n",
            " ['bố' 'má' '0']\n",
            " ['ông_già' 'u' '0']\n",
            " ['sở_trường' 'điểm_yếu' '0']\n",
            " ['tiếp_tục' 'thôi' '0']\n",
            " ['phụ_nữ' 'nam' '0']\n",
            " ['chồng' 'bà_xã' '0']\n",
            " ['phụ_nữ' 'nam_giới' '0']\n",
            " ['sở_trường' 'khuyết_điểm' '0']\n",
            " ['sở_trường' 'hạn_chế' '0']\n",
            " ['nền' 'figure' '0']\n",
            " ['hạ' 'gia_tăng' '0']\n",
            " ['ký_sinh_trùng' 'vật_chủ' '0']\n",
            " ['sở_trường' 'yếu_điểm' '0']\n",
            " ['thuộc_hạ' 'chỉ_huy' '0']\n",
            " ['thầy' 'mẹ' '0']\n",
            " ['chú' 'mợ' '0']\n",
            " ['ông_già' 'mợ' '0']\n",
            " ['phụ_thân' 'me' '0']\n",
            " ['xuất_phát_điểm' 'cuối_cùng' '0']\n",
            " ['bầm' 'mạ' '1']\n",
            " ['mở_đầu' 'trung' '0']\n",
            " ['phục_viên' 'điều_động' '0']\n",
            " ['hoàn_thành' 'bắt_đầu' '0']\n",
            " ['thầy' 'má' '0']\n",
            " ['thầy' 'mợ' '0']\n",
            " ['trụ_cột' 'cột_trụ' '1']\n",
            " ['ông_già' 'má' '0']\n",
            " ['cha' 'má' '0']\n",
            " ['nữ_giới' 'nam' '0']\n",
            " ['tin' 'tin_tức' '1']\n",
            " ['đàn_bà' 'đàn_ông' '0']\n",
            " ['ban_mai' 'hoàng_hôn' '0']\n",
            " ['khu_đô_thị' 'nông_thôn' '0']\n",
            " ['phụ_thân' 'mợ' '0']\n",
            " ['bố' 'mẹ' '0']\n",
            " ['chân_tay' 'lãnh_đạo' '0']\n",
            " ['tay_hữu' 'tay_trái' '0']\n",
            " ['tía' 'má' '0']\n",
            " ['sút' 'gia_tăng' '0']\n",
            " ['tía' 'mẹ' '0']\n",
            " ['quá_khứ' 'sau_này' '0']\n",
            " ['thuộc_hạ' 'lãnh_đạo' '0']\n",
            " ['cậu' 'mợ' '0']\n",
            " ['công_nhân' 'chủ' '0']\n",
            " ['thanh_danh' 'nhục_nhã' '0']\n",
            " ['ba' 'mợ' '0']\n",
            " ['tía' 'mợ' '0']\n",
            " ['phục_viên' 'điều' '0']\n",
            " ['khu_đô_thị' 'đồng_quê' '0']\n",
            " ['chồng' 'hôn_thê' '0']\n",
            " ['vô_địch' 'quán_quân' '1']\n",
            " ['ông_già' 'bầm' '0']\n",
            " ['nữ_giới' 'đàn_ông' '0']\n",
            " ['bố' 'cha' '1']\n",
            " ['ông_già' 'mẹ' '0']\n",
            " ['chồng' 'phu_nhân' '0']\n",
            " ['phụ_thân' 'bầm' '0']\n",
            " ['quá_khứ' 'tương_lai' '0']\n",
            " ['ông_già' 'me' '0']\n",
            " ['chân_tay' 'chỉ_huy' '0']\n",
            " ['ba' 'mẹ' '0']\n",
            " ['giảm_sút' 'gia_tăng' '0']\n",
            " ['dáng_dấp' 'dáng_vẻ' '1']\n",
            " ['bác' 'mợ' '0']\n",
            " ['nữ_giới' 'nam_giới' '0']\n",
            " ['thiên_đường' 'địa_ngục' '0']\n",
            " ['ba' 'má' '0']\n",
            " ['vây_cánh' 'lãnh_đạo' '0']\n",
            " ['phụ_thân' 'mẹ' '0']\n",
            " ['chủ_nghĩa_xã_hội' 'tư_bản_chủ_nghĩa' '0']\n",
            " ['vây_cánh' 'chỉ_huy' '0']\n",
            " ['dĩ_vãng' 'vị_lai' '0']\n",
            " ['cha' 'mẹ' '0']\n",
            " ['chồng' 'vợ' '0']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate with verb\n"
      ],
      "metadata": {
        "id": "giv7VkfrqDQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate with verb\n",
        "evaluate_each_type(vicon400X_verb, vicon400Y_verb,model,'verb')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP7ZaMqvm0E8",
        "outputId": "7f10df02-b492-4aac-d7f0-d45682eadee2"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 3ms/step - loss: 1.0865 - get_f1: 0.8964\n",
            "Type: verb\n",
            "----------------------------------------------------------------------------------------------------\n",
            "precision: 0.8960549817504078\n",
            "recall: 0.8825000000000001\n",
            "f1score: 0.8814859642818869\n",
            "----------------------------------------------------------------------------------------------------\n",
            "wrong prediction:\n",
            "[['giải_thoát' 'thi_hành' '0']\n",
            " ['kìm_giữ' 'trói_buộc' '1']\n",
            " ['lẩn_tránh' 'áp' '0']\n",
            " ['nhận' 'loại_bỏ' '0']\n",
            " ['phản_kháng' 'khuất_phục' '0']\n",
            " ['giả_mạo' 'mạo' '1']\n",
            " ['hạ' 'tăng' '0']\n",
            " ['thanh_trừng' 'phục_chức' '0']\n",
            " ['khuất_núi' 'chào_đời' '0']\n",
            " ['bán' 'sắm' '0']\n",
            " ['ngã' 'bổ' '1']\n",
            " ['phạm' 'giữ' '0']\n",
            " ['thu' 'xoá_bỏ' '0']\n",
            " ['bổ_nhậm' 'miễn_nhiệm' '0']\n",
            " ['khởi_động' 'ngừng' '0']\n",
            " ['bất_an' 'xả' '0']\n",
            " ['trôi' 'đắm' '0']\n",
            " ['chìm' 'nổi' '0']\n",
            " ['tránh_mặt' 'đối_mặt' '0']\n",
            " ['nhập' 'tách' '0']\n",
            " ['va' 'trật' '0']\n",
            " ['nằm' 'ngồi' '0']\n",
            " ['đập' 'hụt' '0']\n",
            " ['quí_mến' 'căm' '0']\n",
            " ['bán' 'mua' '0']\n",
            " ['ca_tụng' 'chỉ_trích' '0']\n",
            " ['thí_điểm' 'thí_nghiệm' '1']\n",
            " ['tắt' 'đóng' '1']\n",
            " ['bật' 'tắt' '0']\n",
            " ['sưu_tập' 'truyền_bá' '0']\n",
            " ['rớt' 'trúng_tuyển' '0']\n",
            " ['lơ' 'lắng_nghe' '0']\n",
            " ['ghi_âm' 'loại_bỏ' '0']\n",
            " ['lấy' 'dán' '0']\n",
            " ['lè' 'thụt' '0']\n",
            " ['tham_dự' 'lỡ' '0']\n",
            " ['băng_hà' 'ra_đời' '0']\n",
            " ['khuyến_khích' 'kiềm_toả' '0']\n",
            " ['công_kích' 'bảo_vệ' '0']\n",
            " ['giải' 'cuộn' '0']\n",
            " ['tháo_dỡ' 'lắp' '0']\n",
            " ['giả_mạo' 'sửa_chữa' '0']\n",
            " ['thăng' 'chào_đời' '0']\n",
            " ['phớt' 'để_tâm' '0']\n",
            " ['khẳng_định' 'từ_bỏ' '0']\n",
            " ['lừa_đảo' 'tỉnh_ngộ' '0']\n",
            " ['trấn_an' 'khuyến_khích' '0']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate with adj\n"
      ],
      "metadata": {
        "id": "AFAMGjVNqFWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluate with adj\n",
        "evaluate_each_type(vicon400X_adj, vicon400Y_adj,model,'adj')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtLjImSQm0QC",
        "outputId": "dc78c73d-7dd9-4074-95d2-ec132a9bfe9d"
      },
      "execution_count": 262,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 0s 2ms/step - loss: 0.4832 - get_f1: 0.9158\n",
            "Type: adj\n",
            "----------------------------------------------------------------------------------------------------\n",
            "precision: 0.9203932603088467\n",
            "recall: 0.9183333333333333\n",
            "f1score: 0.9182331689653158\n",
            "----------------------------------------------------------------------------------------------------\n",
            "wrong prediction:\n",
            "[['đen' 'đỏ' '0']\n",
            " ['gần' 'xa' '0']\n",
            " ['chặt' 'lỏng' '0']\n",
            " ['màu_mỡ' 'phì_nhiêu' '1']\n",
            " ['khô' 'khô_khan' '1']\n",
            " ['nghèo_khó' 'nghèo_túng' '1']\n",
            " ['nườm_nượp' 'vắng_ngắt' '0']\n",
            " ['lớn' 'to' '1']\n",
            " ['bạc' 'tệ' '1']\n",
            " ['dễ' 'khó' '0']\n",
            " ['tỉnh_táo' 'mê_muội' '0']\n",
            " ['nhỡ' 'vừa' '1']\n",
            " ['nhạt_thếch' 'mặn_mòi' '0']\n",
            " ['cực_khổ' 'sung_sướng' '0']\n",
            " ['hà_tiện' 'hoang' '0']\n",
            " ['thường' 'lạ_thường' '0']\n",
            " ['lai' 'thuần_chủng' '0']\n",
            " ['keo_kiệt' 'đá' '1']\n",
            " ['chặt' 'lỏng_lẻo' '0']\n",
            " ['lắm' 'ối' '1']\n",
            " ['mới_tinh' 'mới_toanh' '1']\n",
            " ['thường' 'khác_thường' '0']\n",
            " ['giầu' 'khó' '0']\n",
            " ['bí_mật' 'công_khai' '0']\n",
            " ['khác_biệt' 'tương_đồng' '0']\n",
            " ['chật' 'rộng' '0']\n",
            " ['giầu' 'nghèo' '0']\n",
            " ['chính' 'phụ' '0']\n",
            " ['giàu' 'khó' '0']\n",
            " ['dày' 'mỏng' '0']\n",
            " ['hở' 'kín' '0']\n",
            " ['dại' 'khôn' '0']\n",
            " ['cầu_kì' 'giản_dị' '0']\n",
            " ['rét_mướt' 'ấm_áp' '1']\n",
            " ['dư_dật' 'dư_thừa' '1']\n",
            " ['dài' 'ngắn' '0']\n",
            " ['thẳng' 'cong' '0']\n",
            " ['trong' 'trong_suốt' '1']\n",
            " ['yên_ổn' 'bất_ổn' '0']\n",
            " ['nhạt' 'thẫm' '0']\n",
            " ['đơn' 'kép' '0']\n",
            " ['đại' 'tiểu' '0']\n",
            " ['nông' 'sâu' '0']\n",
            " ['gầy' 'ốm' '1']\n",
            " ['mềm' 'mềm_dẻo' '1']\n",
            " ['anh_dũng' 'hèn_nhát' '0']\n",
            " ['bị_động' 'chủ_động' '0']\n",
            " ['tợn' 'thiện' '0']\n",
            " ['mập' 'ngẳng' '0']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debug in test set"
      ],
      "metadata": {
        "id": "aY-L0YZS9htH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# diff = vicon400Y - y_pred.reshape(-1)\n",
        "# np.where(diff!=0)\n",
        "# tword_pair = np.array(word_pair)\n",
        "# tword_pair[np.where(diff!=0)]\n",
        "# y_pred_pro[np.where(diff!=0)]"
      ],
      "metadata": {
        "id": "dnzkanMG3ilp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate in origin data\n",
        "> Đánh giá mô hình với dữ liệu gốc khi chưa dùng over-sampling\n"
      ],
      "metadata": {
        "id": "AmoBpsntimcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = pd.read_csv('/content/drive/MyDrive/Colab/NLP/Word-Similarity/word2vec/e300.csv')\n",
        "embedding = w2v.iloc[:, : 300].to_numpy()\n",
        "embedding = dict(zip(w2v['word'],embedding))\n",
        "make_unique = {}\n",
        "vicon400X_noun,vicon400Y_noun,vicon400X_verb,vicon400Y_verb,vicon400X_adj,vicon400Y_adj , vicon400X,vicon400Y ,  word_pair_noun,word_pair_verb,word_pair_adj = readTest()\n",
        "X,Y = readInput(isFull=False)\n",
        "\n",
        "#print(X.shape)\n",
        "def eveluate_origin_data():\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Dense(100, activation = 'relu'),\n",
        "                               tf.keras.layers.Dense(10, activation = 'relu'),\n",
        "                               tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "                              ])\n",
        "  model.compile( loss= tf.keras.losses.binary_crossentropy,\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01),\n",
        "                metrics = [get_f1])\n",
        "  model.fit(X, Y, epochs = 35, verbose = True)\n",
        "  y_pred = model.predict(vicon400X)\n",
        "  y_pred_pro = copy.deepcopy(y_pred)\n",
        "  y_pred[y_pred>0.5] = 1\n",
        "  y_pred[y_pred<=0.5] = 0\n",
        "  precision, recall, fscore, support = score(vicon400Y, y_pred , average='macro')\n",
        "  print('precision: {}'.format(precision))\n",
        "  print('recall: {}'.format(recall))\n",
        "  print('f1score: {}'.format(fscore))\n",
        "  # print('support: {}'.format(support))\n",
        "\n",
        "eveluate_origin_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-snsKUURAetQ",
        "outputId": "ea45866c-387a-430e-dee0-2c0ce11ea295"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "360/360 [==============================] - 2s 3ms/step - loss: 0.1951 - get_f1: 0.9663\n",
            "Epoch 2/35\n",
            "360/360 [==============================] - 1s 4ms/step - loss: 0.1220 - get_f1: 0.9751\n",
            "Epoch 3/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0770 - get_f1: 0.9862\n",
            "Epoch 4/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0480 - get_f1: 0.9913\n",
            "Epoch 5/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0314 - get_f1: 0.9951\n",
            "Epoch 6/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0284 - get_f1: 0.9957\n",
            "Epoch 7/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0199 - get_f1: 0.9967\n",
            "Epoch 8/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0107 - get_f1: 0.9981\n",
            "Epoch 9/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0140 - get_f1: 0.9978\n",
            "Epoch 10/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0121 - get_f1: 0.9979\n",
            "Epoch 11/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0108 - get_f1: 0.9979\n",
            "Epoch 12/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0150 - get_f1: 0.9973\n",
            "Epoch 13/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0117 - get_f1: 0.9981\n",
            "Epoch 14/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0109 - get_f1: 0.9986\n",
            "Epoch 15/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0134 - get_f1: 0.9982\n",
            "Epoch 16/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0098 - get_f1: 0.9984\n",
            "Epoch 17/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0042 - get_f1: 0.9994\n",
            "Epoch 18/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0097 - get_f1: 0.9990\n",
            "Epoch 19/35\n",
            "360/360 [==============================] - 1s 4ms/step - loss: 0.0075 - get_f1: 0.9991\n",
            "Epoch 20/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0051 - get_f1: 0.9993\n",
            "Epoch 21/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0049 - get_f1: 0.9991\n",
            "Epoch 22/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0030 - get_f1: 0.9991\n",
            "Epoch 23/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0059 - get_f1: 0.9993\n",
            "Epoch 24/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0324 - get_f1: 0.9955\n",
            "Epoch 25/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0162 - get_f1: 0.9975\n",
            "Epoch 26/35\n",
            "360/360 [==============================] - 1s 4ms/step - loss: 0.0074 - get_f1: 0.9989\n",
            "Epoch 27/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0035 - get_f1: 0.9993\n",
            "Epoch 28/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0033 - get_f1: 0.9994\n",
            "Epoch 29/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0021 - get_f1: 0.9994\n",
            "Epoch 30/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0025 - get_f1: 0.9994\n",
            "Epoch 31/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0046 - get_f1: 0.9992\n",
            "Epoch 32/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0087 - get_f1: 0.9984\n",
            "Epoch 33/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0103 - get_f1: 0.9974\n",
            "Epoch 34/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0079 - get_f1: 0.9988\n",
            "Epoch 35/35\n",
            "360/360 [==============================] - 1s 3ms/step - loss: 0.0106 - get_f1: 0.9992\n",
            "precision: 0.8069277988892136\n",
            "recall: 0.7121428571428572\n",
            "f1score: 0.6880596280489795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evalue in 50 dimensions embedding\n"
      ],
      "metadata": {
        "id": "fafJLSay_fwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = pd.read_csv('/content/drive/MyDrive/Colab/NLP/Word-Similarity/word2vec/e50.csv')\n",
        "embedding = w2v.iloc[:, : 50].to_numpy()\n",
        "embedding = dict(zip(w2v['word'],embedding))\n",
        "make_unique = {}\n",
        "vicon400X_noun,vicon400Y_noun,vicon400X_verb,vicon400Y_verb,vicon400X_adj,vicon400Y_adj , vicon400X,vicon400Y , word_pair_noun,word_pair_verb,word_pair_adj = readTest()\n",
        "X,Y = readInput(isFull=True)\n",
        "def eveluate_in_50_dimensions_data():\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Dense(100, activation = 'relu'),\n",
        "                               tf.keras.layers.Dense(10, activation = 'relu'),\n",
        "                               tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "                              ])\n",
        "  model.compile( loss= tf.keras.losses.binary_crossentropy,\n",
        "                optimizer = tf.keras.optimizers.Adam(learning_rate = 0.01),\n",
        "                metrics = [get_f1])\n",
        "  model.fit(X, Y, epochs = 35, verbose = True)\n",
        "  y_pred = model.predict(vicon400X)\n",
        "  y_pred_pro = copy.deepcopy(y_pred)\n",
        "  y_pred[y_pred>0.5] = 1\n",
        "  y_pred[y_pred<=0.5] = 0\n",
        "  precision, recall, fscore, support = score(vicon400Y, y_pred , average='macro')\n",
        "  print('precision: {}'.format(precision))\n",
        "  print('recall: {}'.format(recall))\n",
        "  print('f1score: {}'.format(fscore))\n",
        "  # print('support: {}'.format(support))\n",
        "\n",
        "eveluate_in_50_dimensions_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV7jZcWO_knM",
        "outputId": "c1de18e8-c337-40ad-fa58-d1b138c1ef3b"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "1327/1327 [==============================] - 7s 4ms/step - loss: 0.2795 - get_f1: 0.9400\n",
            "Epoch 2/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.2142 - get_f1: 0.9521\n",
            "Epoch 3/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.1844 - get_f1: 0.9588\n",
            "Epoch 4/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.1658 - get_f1: 0.9620\n",
            "Epoch 5/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.1498 - get_f1: 0.9651\n",
            "Epoch 6/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.1344 - get_f1: 0.9684\n",
            "Epoch 7/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.1251 - get_f1: 0.9712\n",
            "Epoch 8/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.1136 - get_f1: 0.9741\n",
            "Epoch 9/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.1081 - get_f1: 0.9747\n",
            "Epoch 10/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.1023 - get_f1: 0.9761\n",
            "Epoch 11/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0963 - get_f1: 0.9775\n",
            "Epoch 12/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0916 - get_f1: 0.9792\n",
            "Epoch 13/35\n",
            "1327/1327 [==============================] - 5s 3ms/step - loss: 0.0878 - get_f1: 0.9800\n",
            "Epoch 14/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0805 - get_f1: 0.9818\n",
            "Epoch 15/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0788 - get_f1: 0.9820\n",
            "Epoch 16/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0757 - get_f1: 0.9831\n",
            "Epoch 17/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0714 - get_f1: 0.9835\n",
            "Epoch 18/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0702 - get_f1: 0.9836\n",
            "Epoch 19/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0651 - get_f1: 0.9848\n",
            "Epoch 20/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0643 - get_f1: 0.9850\n",
            "Epoch 21/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0608 - get_f1: 0.9865\n",
            "Epoch 22/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0597 - get_f1: 0.9865\n",
            "Epoch 23/35\n",
            "1327/1327 [==============================] - 5s 3ms/step - loss: 0.0568 - get_f1: 0.9871\n",
            "Epoch 24/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0555 - get_f1: 0.9871\n",
            "Epoch 25/35\n",
            "1327/1327 [==============================] - 5s 4ms/step - loss: 0.0529 - get_f1: 0.9881\n",
            "Epoch 26/35\n",
            "1327/1327 [==============================] - 5s 4ms/step - loss: 0.0524 - get_f1: 0.9879\n",
            "Epoch 27/35\n",
            "1327/1327 [==============================] - 5s 4ms/step - loss: 0.0505 - get_f1: 0.9888\n",
            "Epoch 28/35\n",
            "1327/1327 [==============================] - 5s 4ms/step - loss: 0.0513 - get_f1: 0.9885\n",
            "Epoch 29/35\n",
            "1327/1327 [==============================] - 5s 3ms/step - loss: 0.0496 - get_f1: 0.9889\n",
            "Epoch 30/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0489 - get_f1: 0.9892\n",
            "Epoch 31/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0437 - get_f1: 0.9902\n",
            "Epoch 32/35\n",
            "1327/1327 [==============================] - 5s 3ms/step - loss: 0.0419 - get_f1: 0.9905\n",
            "Epoch 33/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0451 - get_f1: 0.9900\n",
            "Epoch 34/35\n",
            "1327/1327 [==============================] - 4s 3ms/step - loss: 0.0395 - get_f1: 0.9913\n",
            "Epoch 35/35\n",
            "1327/1327 [==============================] - 5s 4ms/step - loss: 0.0397 - get_f1: 0.9912\n",
            "precision: 0.8302955133735979\n",
            "recall: 0.8\n",
            "f1score: 0.7953062469191501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "48vFeblkKj2K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}